{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6bae11",
   "metadata": {},
   "outputs": [],
   "source": [
    "Scrape the details of most viewed videos on YouTube from Wikipedia. Url \n",
    "= https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos You need to find following details:\n",
    "A)Rank \n",
    "B) Name \n",
    "C) Artist \n",
    "D) Upload date \n",
    "E) Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed61cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos\"\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    videos_section = soup.find('table', {'class': 'wikitable'})\n",
    "\n",
    "    if videos_section:\n",
    "        rows = videos_section.find_all('tr')[1:]  \n",
    "\n",
    "        for row in rows:\n",
    "            columns = row.find_all(['th', 'td'])\n",
    "\n",
    "            if len(columns) >= 5:\n",
    "                rank = columns[0].text.strip()\n",
    "                name = columns[1].text.strip()\n",
    "                artist = columns[2].text.strip()\n",
    "                upload_date = columns[3].text.strip()\n",
    "                views = columns[4].text.strip()\n",
    "\n",
    "                print(f\"Rank: {rank}\\nName: {name}\\nArtist: {artist}\\nUpload Date: {upload_date}\\nViews: {views}\\n---\\n\")\n",
    "            else:\n",
    "                print(\"Skipping row with insufficient columns.\")\n",
    "    else:\n",
    "        print(\"Couldn't find the section containing the video information.\")\n",
    "else:\n",
    "    print(f\"Failed to retrieve the Wikipedia page. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfe8a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "Scrape the details team Indiaâ€™s international fixtures from bcci.tv. \n",
    "Url = https://www.bcci.tv/. \n",
    "You need to find following details: \n",
    "A) Series \n",
    "B) Place \n",
    "C) Date \n",
    "D) Time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2405ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "homepage_url = \"https://www.bcci.tv/\"\n",
    "response_homepage = requests.get(homepage_url)\n",
    "\n",
    "if response_homepage.status_code == 200:\n",
    "    soup_homepage = BeautifulSoup(response_homepage.text, 'html.parser')\n",
    "\n",
    "    fixture_link = soup_homepage.find('a', {'data-nav-index': '0', 'data-match-state': 'Fixture'})\n",
    "\n",
    "    if fixture_link:\n",
    "        fixture_url = \"https://www.bcci.tv\" + fixture_link['href']\n",
    "\n",
    "        response_fixture = requests.get(fixture_url)\n",
    "\n",
    "        if response_fixture.status_code == 200:\n",
    "            soup_fixture = BeautifulSoup(response_fixture.text, 'html.parser')\n",
    "\n",
    "            fixtures_section = soup_fixture.find('section', {'class': 'js-list'})\n",
    "\n",
    "            fixtures = fixtures_section.find_all('li', {'class': 'event-list__item'})\n",
    "\n",
    "            for fixture in fixtures:\n",
    "                series = fixture.find('div', {'class': 'event-list__fixture'}).get_text(strip=True)\n",
    "                place = fixture.find('p', {'class': 'event-list__location'}).get_text(strip=True)\n",
    "                date = fixture.find('span', {'class': 'fixture__datetime'}).get('data-date')\n",
    "                time = fixture.find('span', {'class': 'fixture__datetime'}).get('data-time')\n",
    "\n",
    "                print(f\"Series: {series}\\nPlace: {place}\\nDate: {date}\\nTime: {time}\\n---\\n\")\n",
    "\n",
    "        else:\n",
    "            print(f\"Failed to retrieve the international fixture page. Status code: {response_fixture.status_code}\")\n",
    "    else:\n",
    "        print(\"Link to the international fixture page not found on the homepage.\")\n",
    "else:\n",
    "    print(f\"Failed to retrieve the homepage. Status code: {response_homepage.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acaa518",
   "metadata": {},
   "outputs": [],
   "source": [
    "Scrape the details of State-wise GDP of India from statisticstime.com. \n",
    "Url = http://statisticstimes.com/ \n",
    "You have to find following details: A) Rank \n",
    "B) State \n",
    "C) GSDP(18-19)- at current prices \n",
    "D) GSDP(19-20)- at current prices \n",
    "E) Share(18-19) \n",
    "F) GDP($ billion) \n",
    "Note: - From statisticstimes home page you have to reach to economy page through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7aa55c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "homepage_url = \"http://statisticstimes.com/\"\n",
    "response_homepage = requests.get(homepage_url)\n",
    "\n",
    "if response_homepage.status_code == 200:\n",
    "    soup_homepage = BeautifulSoup(response_homepage.text, 'html.parser')\n",
    "\n",
    "    economy_link = soup_homepage.find('a', {'title': 'Economy'})\n",
    "\n",
    "    if economy_link:\n",
    "        economy_url = homepage_url + economy_link['href']\n",
    "\n",
    "        response_economy = requests.get(economy_url)\n",
    "\n",
    "        if response_economy.status_code == 200:\n",
    "            soup_economy = BeautifulSoup(response_economy.text, 'html.parser')\n",
    "\n",
    "            gdp_table = soup_economy.find('table', {'class': 'display'})\n",
    "\n",
    "            rows = gdp_table.find_all('tr')[1:]\n",
    "\n",
    "            for row in rows:\n",
    "                columns = row.find_all('td')\n",
    "                \n",
    "                rank = columns[0].get_text(strip=True)\n",
    "                state = columns[1].get_text(strip=True)\n",
    "                gsdp_1819 = columns[2].get_text(strip=True)\n",
    "                gsdp_1920 = columns[3].get_text(strip=True)\n",
    "                share_1819 = columns[4].get_text(strip=True)\n",
    "                gdp_billion = columns[5].get_text(strip=True)\n",
    "\n",
    "                print(f\"Rank: {rank}\\nState: {state}\\nGSDP(18-19): {gsdp_1819}\\nGSDP(19-20): {gsdp_1920}\\nShare(18-19): {share_1819}\\nGDP($ billion): {gdp_billion}\\n---\\n\")\n",
    "\n",
    "        else:\n",
    "            print(f\"Failed to retrieve the economy page. Status code: {response_economy.status_code}\")\n",
    "    else:\n",
    "        print(\"Link to the economy page not found on the homepage.\")\n",
    "else:\n",
    "    print(f\"Failed to retrieve the homepage. Status code: {response_homepage.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfb5a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "Scrape the details of trending repositories on Github.com. \n",
    "Url = https://github.com/ \n",
    "You have to find the following details: \n",
    "A) Repository title \n",
    "B) Repository description \n",
    "C) Contributors count \n",
    "D) Language used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048647ba",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://github.com/\"\n",
    "\n",
    "response_homepage = requests.get(url)\n",
    "\n",
    "if response_homepage.status_code == 200:\n",
    "    soup_homepage = BeautifulSoup(response_homepage.text, 'html.parser')\n",
    "\n",
    "    trending_link = soup_homepage.find('a', {'href': '/trending'})\n",
    "\n",
    "    if trending_link:\n",
    "        trending_url = \"https://github.com\" + trending_link['href']\n",
    "\n",
    "        response_trending = requests.get(trending_url)\n",
    "\n",
    "        if response_trending.status_code == 200:\n",
    "            soup_trending = BeautifulSoup(response_trending.text, 'html.parser')\n",
    "\n",
    "            repos_section = soup_trending.find_all('article', {'class': 'Box-row'})\n",
    "\n",
    "            for repo in repos_section:\n",
    "                title_elem = repo.find('h1', {'class': 'h3 lh-condensed'})\n",
    "                title = title_elem.find('a').text.strip() if title_elem else \"N/A\"\n",
    "\n",
    "                description_elem = repo.find('p', {'class': 'col-9 color-text-secondary my-1 pr-4'})\n",
    "                description = description_elem.text.strip() if description_elem else \"N/A\"\n",
    "\n",
    "                contributors_elem = repo.find('a', {'href': title + '/graphs/contributors'})\n",
    "                contributors_count = contributors_elem.text.strip() if contributors_elem else \"N/A\"\n",
    "\n",
    "                language_elem = repo.find('span', {'itemprop': 'programmingLanguage'})\n",
    "                language = language_elem.text.strip() if language_elem else \"N/A\"\n",
    "\n",
    "                print(f\"Title: {title}\\nDescription: {description}\\nContributors Count: {contributors_count}\\nLanguage: {language}\\n---\\n\")\n",
    "\n",
    "        else:\n",
    "            print(f\"Failed to retrieve the trending page. Status code: {response_trending.status_code}\")\n",
    "    else:\n",
    "        print(\"Link to the trending page not found on the homepage.\")\n",
    "else:\n",
    "    print(f\"Failed to retrieve the homepage. Status code: {response_homepage.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2633210c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Scrape the details of top 100 songs on billiboard.com. Url = https:/www.billboard.com/ You have to find the \n",
    "following details: \n",
    "A) Song name \n",
    "B) Artist name \n",
    "C) Last week rank \n",
    "D) Peak rank \n",
    "E) Weeks on board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f176ba5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.billboard.com/charts/hot-100/\"\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    songs_section = soup.find('div', {'class': 'chart-list'})\n",
    "\n",
    "    if songs_section:\n",
    "        songs = songs_section.find_all('li', {'class': 'chart-list__element'})\n",
    "\n",
    "        for song in songs:\n",
    "            song_name = song.find('span', {'class': 'chart-element__information__song text--truncate color--primary'}).text.strip()\n",
    "            artist_name = song.find('span', {'class': 'chart-element__information__artist text--truncate color--secondary'}).text.strip()\n",
    "            last_week_rank = song.find('span', {'class': 'chart-element__meta text--center color--secondary text--last'}).text.strip()\n",
    "            peak_rank = song.find('span', {'class': 'chart-element__meta text--center color--secondary text--peak'}).text.strip()\n",
    "            weeks_on_board = song.find('span', {'class': 'chart-element__meta text--center color--secondary text--week'}).text.strip()\n",
    "\n",
    "            print(f\"Song Name: {song_name}\\nArtist Name: {artist_name}\\nLast Week Rank: {last_week_rank}\\nPeak Rank: {peak_rank}\\nWeeks on Board: {weeks_on_board}\\n---\\n\")\n",
    "    else:\n",
    "        print(\"Couldn't find the section containing the top songs.\")\n",
    "else:\n",
    "    print(f\"Failed to retrieve the Billboard Hot 100 page. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d3ba92",
   "metadata": {},
   "outputs": [],
   "source": [
    "Scrape the details of Highest selling novels. \n",
    "A) Book name \n",
    "B) Author name \n",
    "C) Volumes sold \n",
    "D) Publisher \n",
    "E) Genre \n",
    " Url - https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b065c501",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare\"\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    table = soup.find('table', {'class': 'in-article sortable'})\n",
    "\n",
    "    if table:\n",
    "        rows = table.find_all('tr')[1:]  row\n",
    "\n",
    "        for row in rows:\n",
    "            columns = row.find_all('td')\n",
    "\n",
    "            if len(columns) >= 5:\n",
    "                book_name = columns[0].text.strip()\n",
    "                author_name = columns[1].text.strip()\n",
    "                volumes_sold = columns[2].text.strip()\n",
    "                publisher = columns[3].text.strip()\n",
    "                genre = columns[4].text.strip()\n",
    "\n",
    "                print(f\"Book Name: {book_name}\\nAuthor Name: {author_name}\\nVolumes Sold: {volumes_sold}\\nPublisher: {publisher}\\nGenre: {genre}\\n---\\n\")\n",
    "            else:\n",
    "                print(\"Skipping row with insufficient columns.\")\n",
    "    else:\n",
    "        print(\"Couldn't find the table containing the information.\")\n",
    "else:\n",
    "    print(f\"Failed to retrieve the webpage. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda01aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "Scrape the details most watched tv series of all time from imdb.com. \n",
    "Url = https://www.imdb.com/list/ls095964455/ You have \n",
    "to find the following details: \n",
    "A) Name \n",
    "B) Year span \n",
    "C) Genre \n",
    "D) Run time \n",
    "E) Ratings \n",
    "F) Votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13588782",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.imdb.com/list/ls095964455/\"\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    series_section = soup.find('div', {'class': 'lister-list'})\n",
    "\n",
    "    if series_section:\n",
    "        series_items = series_section.find_all('div', {'class': 'lister-item mode-detail'})\n",
    "\n",
    "        for series_item in series_items:\n",
    "            name = series_item.find('h3', {'class': 'lister-item-header'}).find('a').text.strip()\n",
    "            year_span = series_item.find('span', {'class': 'lister-item-year'}).text.strip()\n",
    "            genre = series_item.find('span', {'class': 'genre'}).text.strip()\n",
    "            run_time = series_item.find('span', {'class': 'runtime'}).text.strip()\n",
    "            ratings = series_item.find('span', {'class': 'ipl-rating-star__rating'}).text.strip()\n",
    "            votes = series_item.find('span', {'name': 'nv'}).text.strip()\n",
    "\n",
    "            print(f\"Name: {name}\\nYear Span: {year_span}\\nGenre: {genre}\\nRun Time: {run_time}\\nRatings: {ratings}\\nVotes: {votes}\\n---\\n\")\n",
    "    else:\n",
    "        print(\"Couldn't find the section containing the information.\")\n",
    "else:\n",
    "    print(f\"Failed to retrieve the webpage. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2280274d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba28aeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "url_home = \"https://archive.ics.uci.edu/\"\n",
    "response_home = requests.get(url_home)\n",
    "\n",
    "if response_home.status_code == 200:\n",
    "    soup_home = BeautifulSoup(response_home.text, 'html.parser')\n",
    "\n",
    "    link_show_all = soup_home.find('a', {'href': '/ml/datasets.php'})\n",
    "\n",
    "    if link_show_all:\n",
    "        url_show_all = urljoin(url_home, link_show_all['href'])\n",
    "\n",
    "        response_show_all = requests.get(url_show_all)\n",
    "\n",
    "        if response_show_all.status_code == 200:\n",
    "            soup_show_all = BeautifulSoup(response_show_all.text, 'html.parser')\n",
    "\n",
    "            dataset_section = soup_show_all.find('table', {'border': '1', 'cellspacing': '0', 'cellpadding': '5'})\n",
    "\n",
    "            if dataset_section:\n",
    "                rows = dataset_section.find_all('tr')[1:]  \n",
    "\n",
    "                for row in rows:\n",
    "                    columns = row.find_all('td')\n",
    "\n",
    "                    dataset_name = columns[0].text.strip()\n",
    "                    data_type = columns[1].text.strip()\n",
    "                    task = columns[2].text.strip()\n",
    "                    attribute_type = columns[3].text.strip()\n",
    "                    no_of_instances = columns[4].text.strip()\n",
    "                    no_of_attributes = columns[5].text.strip()\n",
    "                    year = columns[6].text.strip()\n",
    "\n",
    "                    print(f\"Dataset Name: {dataset_name}\\nData Type: {data_type}\\nTask: {task}\\nAttribute Type: {attribute_type}\\n\"\n",
    "                          f\"No of Instances: {no_of_instances}\\nNo of Attributes: {no_of_attributes}\\nYear: {year}\\n---\\n\")\n",
    "            else:\n",
    "                print(\"Couldn't find the section containing the dataset information.\")\n",
    "        else:\n",
    "            print(f\"Failed to retrieve the Show All Dataset page. Status code: {response_show_all.status_code}\")\n",
    "    else:\n",
    "        print(\"Link to the Show All Dataset page not found on the home page.\")\n",
    "else:\n",
    "    print(f\"Failed to retrieve the home page. Status code: {response_home.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed92ed60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
